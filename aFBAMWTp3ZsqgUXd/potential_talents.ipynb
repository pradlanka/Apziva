{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Project3: Potential Talents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background:\n",
    "\n",
    "As a talent sourcing and management company, we are interested in finding talented individuals for sourcing these candidates to technology companies. Finding talented candidates is not easy, for several reasons. The first reason is one needs to understand what the role is very well to fill in that spot, this requires understanding the client’s needs and what they are looking for in a potential candidate. The second reason is one needs to understand what makes a candidate shine for the role we are in search for. Third, where to find talented individuals is another challenge.\n",
    "\n",
    "The nature of our job requires a lot of human labor and is full of manual operations. Towards automating this process we want to build a better approach that could save us time and finally help us spot potential candidates that could fit the roles we are in search for. Moreover, going beyond that for a specific role we want to fill in we are interested in developing a machine learning powered pipeline that could spot talented individuals, and rank them based on their fitness.\n",
    "\n",
    "We are right now semi-automatically sourcing a few candidates, therefore the sourcing part is not a concern at this time but we expect to first determine best matching candidates based on how fit these candidates are for a given role. We generally make these searches based on some keywords such as “full-stack software engineer”, “engineering manager” or “aspiring human resources” based on the role we are trying to fill in. These keywords might change, and you can expect that specific keywords will be provided to you.\n",
    "\n",
    "Assuming that we were able to list and rank fitting candidates, we then employ a review procedure, as each candidate needs to be reviewed and then determined how good a fit they are through manual inspection. This procedure is done manually and at the end of this manual review, we might choose not the first fitting candidate in the list but maybe the 7th candidate in the list. If that happens, we are interested in being able to re-rank the previous list based on this information. This supervisory signal is going to be supplied by starring the 7th candidate in the list. Starring one candidate actually sets this candidate as an ideal candidate for the given role. Then, we expect the list to be re-ranked each time a candidate is starred.\n",
    "\n",
    "Data Description:\n",
    "\n",
    "The data comes from our sourcing efforts. We removed any field that could directly reveal personal details and gave a unique identifier for each candidate.\n",
    "\n",
    "Attributes:\n",
    "id : unique identifier for candidate (numeric)\n",
    "\n",
    "job_title : job title for candidate (text)\n",
    "\n",
    "location : geographical location for candidate (text)\n",
    "\n",
    "connections: number of connections candidate has, 500+ means over 500 (text)\n",
    "\n",
    "Output (desired target):\n",
    "fit - how fit the candidate is for the role? (numeric, probability between 0-1)\n",
    "\n",
    "Keywords: “Aspiring human resources” or “seeking human resources”\n",
    "\n",
    "Download Data:\n",
    "\n",
    "https://docs.google.com/spreadsheets/d/117X6i53dKiO7w6kuA1g1TpdTlv1173h_dPlJt5cNNMU/edit?usp=sharing\n",
    "\n",
    "Goal(s):\n",
    "\n",
    "Predict how fit the candidate is based on their available information (variable fit)\n",
    "\n",
    "Success Metric(s):\n",
    "\n",
    "Rank candidates based on a fitness score.\n",
    "\n",
    "Re-rank candidates when a candidate is starred.\n",
    "\n",
    "Bonus(es):\n",
    "\n",
    "We are interested in a robust algorithm, tell us how your solution works and show us how your ranking gets better with each starring action.\n",
    "\n",
    "How can we filter out candidates which in the first place should not be in this list?\n",
    "\n",
    "Can we determine a cut-off point that would work for other roles without losing high potential candidates?\n",
    "\n",
    "Do you have any ideas that we should explore so that we can even automate this procedure to prevent human bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the necessary packages and the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could use spaCy to tokenize and stemming our keywords and job searches\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104, 5)\n",
      "|    |   id | job_title                                                                                                | location                            | connection   |   fit |\n",
      "|---:|-----:|:---------------------------------------------------------------------------------------------------------|:------------------------------------|:-------------|------:|\n",
      "|  0 |    1 | 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional | Houston, Texas                      | 85           |   nan |\n",
      "|  1 |    2 | Native English Teacher at EPIK (English Program in Korea)                                                | Kanada                              | 500+         |   nan |\n",
      "|  2 |    3 | Aspiring Human Resources Professional                                                                    | Raleigh-Durham, North Carolina Area | 44           |   nan |\n",
      "|  3 |    4 | People Development Coordinator at Ryan                                                                   | Denton, Texas                       | 500+         |   nan |\n",
      "|  4 |    5 | Advisory Board Member at Celal Bayar University                                                          | İzmir, Türkiye                      | 500+         |   nan |\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "talents_df =  pd.read_excel('potential-talents.xlsx')\n",
    "print(talents_df.shape)\n",
    "print(talents_df.head().to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data frame  it is clear that there area few duplicates. Lets remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53, 4)\n",
      "|    | job_title                                                                                                | location                            | connection   |   fit |\n",
      "|---:|:---------------------------------------------------------------------------------------------------------|:------------------------------------|:-------------|------:|\n",
      "|  0 | 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional | Houston, Texas                      | 85           |   nan |\n",
      "|  1 | Native English Teacher at EPIK (English Program in Korea)                                                | Kanada                              | 500+         |   nan |\n",
      "|  2 | Aspiring Human Resources Professional                                                                    | Raleigh-Durham, North Carolina Area | 44           |   nan |\n",
      "|  3 | People Development Coordinator at Ryan                                                                   | Denton, Texas                       | 500+         |   nan |\n",
      "|  4 | Advisory Board Member at Celal Bayar University                                                          | İzmir, Türkiye                      | 500+         |   nan |\n"
     ]
    }
   ],
   "source": [
    "# Clean the duplicates in the data\n",
    "talents_df_clean = talents_df.drop(columns= 'id').drop_duplicates().reset_index(drop= True)\n",
    "print(talents_df_clean.shape)\n",
    "print(talents_df_clean.head().to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Modeling\n",
    "We plan to use 3 ways to build the model\n",
    "1. Look for similar words between the search term and candidate job title extracted from this profile.  We can additionally use location information and connections to improve the ranking. Here we implement this using spacy to tokenize and stem the words, then we can use the tf-idf vectorizer to vectorize our words and use either jacquard similarity or cosine transformation to find the similarity between the search term and candidate job title.\n",
    "\n",
    "Furthermore we can hard-code the importance of the no of connections by encoding them as ordingal classes with\n",
    "\n",
    "<100: 'Low'; 101-200: 'Average'; 200-500: 'High'; 500+: 'Very-high'\n",
    "\n",
    "\n",
    "As for location, we can extract the city, state and country information from the 'location' column and match if it is the same location as the job search keyword and assign a score accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0 Just looking at similar words  between the \"search term keywords\" and the \"job profile\"  and quanitfying similarity through jacquard simialrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | job_title                                                              | location                            |   connection |      fit |\n",
      "|---:|:-----------------------------------------------------------------------|:------------------------------------|-------------:|---------:|\n",
      "| 45 | Aspiring Human Resources Professional                                  | Kokomo, Indiana Area                |           71 | 0.666667 |\n",
      "|  2 | Aspiring Human Resources Professional                                  | Raleigh-Durham, North Carolina Area |           44 | 0.666667 |\n",
      "| 13 | Seeking Human Resources Opportunities                                  | Chicago, Illinois                   |          390 | 0.666667 |\n",
      "|  5 | Aspiring Human Resources Specialist                                    | Greater New York City Area          |            1 | 0.666667 |\n",
      "| 42 | Seeking Human  Resources Opportunities. Open to travel and relocation. | Amerika Birleşik Devletleri         |          415 | 0.4      |\n",
      "|  6 | Student at Humber College and Aspiring Human Resources Generalist      | Kanada                              |           61 | 0.333333 |\n",
      "| 20 | Business Management Major and Aspiring Human Resources Manager         | Monroe, Louisiana Area              |            5 | 0.333333 |\n",
      "| 47 | Seeking Human Resources Position                                       | Las Vegas, Nevada Area              |           48 | 0.25     |\n",
      "| 22 | Human Resources Professional                                           | Greater Boston Area                 |           16 | 0.25     |\n",
      "| 36 | Human Resources Management Major                                       | Milpitas, California                |           18 | 0.2      |\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class TextSimilarityTransformerJS(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, search_words, spacy_nlp):\n",
    "        \"\"\"\n",
    "        Custom transformer to calculate TF-IDF cosine similarity \n",
    "        between a reference text and a series of input texts.\n",
    "        \"\"\"\n",
    "        self.search_words = search_words\n",
    "        self.spacy_nlp = spacy_nlp\n",
    " \n",
    "\n",
    "    def spacy_tokenize_lemmatize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenizes and lemmatizes text using spaCy.\n",
    "        \"\"\"\n",
    "        if (text) or (text is not None):\n",
    "            return([w.lemma_.lower() for w in self.spacy_nlp(text) if w.pos_ in ['NOUN','PROPN']])\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    # Defining jacquard similarity\n",
    "    def job_similarity_kw_js(self, job_title, search_words):\n",
    "        job_title_token = set(self.spacy_tokenize_lemmatize(job_title))\n",
    "        search_words_token = set(self.spacy_tokenize_lemmatize(search_words))\n",
    "        return(len(job_title_token.intersection(search_words_token))/len(job_title_token.union(search_words_token)))\n",
    "\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Transform the input texts to TF-IDF vectors\n",
    "    \n",
    "        # Calculate cosine similarity between the reference text and each input text\n",
    "        similarity_scores = X.apply(self.job_similarity_kw_js, search_words= self.search_words)\n",
    "    \n",
    "        return similarity_scores\n",
    "    \n",
    "# Sample DataFrame and reference sentence\n",
    "search_words = \"Aspiring Human Resources\"\n",
    "\n",
    "# Initialize and apply the transformer\n",
    "similarity_transformer = TextSimilarityTransformerJS(search_words =search_words,  spacy_nlp=nlp)\n",
    "talents_df_clean['fit']  = similarity_transformer.fit_transform(talents_df_clean['job_title'])\n",
    "\n",
    "# Printing out the top 10 candidates based on the highest fit score\n",
    "print(talents_df_clean.sort_values(by =['fit'], ascending=False, inplace = False).head(10).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Using N-Gram models and Tf-Idf Vectorizers and using cosine transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | job_title                                                        | location                            | connection   |      fit |\n",
      "|---:|:-----------------------------------------------------------------|:------------------------------------|:-------------|---------:|\n",
      "| 25 | Human Resources|                                                 | Dallas/Fort Worth Area              | 409          | 0.569669 |\n",
      "|    | Conflict Management|                                             |                                     |              |          |\n",
      "|    | Policies & Procedures|Talent Management|Benefits & Compensation  |                                     |              |          |\n",
      "| 36 | Human Resources Management Major                                 | Milpitas, California                | 18           | 0.569669 |\n",
      "| 17 | Director of Human Resources North America, Groupe Beneteau       | Greater Grand Rapids, Michigan Area | 500+         | 0.569669 |\n",
      "| 26 | Human Resources Generalist at Schwan's                           | Amerika Birleşik Devletleri         | 500+         | 0.458555 |\n",
      "| 11 | Human Resources Coordinator at InterContinental Buckhead Atlanta | Atlanta, Georgia                    | 500+         | 0.458555 |\n",
      "| 19 | Human Resources Generalist at ScottMadden, Inc.                  | Raleigh-Durham, North Carolina Area | 500+         | 0.458555 |\n",
      "| 16 | Human Resources Specialist at Luxottica                          | Greater New York City Area          | 500+         | 0.458555 |\n",
      "| 29 | Senior Human Resources Business Partner at Heil Environmental    | Chattanooga, Tennessee Area         | 455          | 0.458555 |\n",
      "| 37 | Director Human Resources  at EY                                  | Greater Atlanta Area                | 349          | 0.458555 |\n",
      "| 49 | Human Resources Generalist at Loparex                            | Raleigh-Durham, North Carolina Area | 500+         | 0.458555 |\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "    \n",
    "class TextSimilarityTransformerCS(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, search_words, spacy_nlp):\n",
    "        \"\"\"\n",
    "        Custom transformer to calculate TF-IDF cosine similarity \n",
    "        between a reference text and a series of input texts.\n",
    "        \"\"\"\n",
    "        self.search_words = search_words\n",
    "        self.spacy_nlp = spacy_nlp\n",
    "        stop_words_tokenized = set(self.spacy_tokenize_lemmatize(' '.join(sorted(STOP_WORDS))))\n",
    "        self.vectorizer =  TfidfVectorizer(min_df =0.1, max_features=100, \n",
    "                                    stop_words= list(stop_words_tokenized), ngram_range = (1,4))\n",
    "        \n",
    "\n",
    "    def spacy_tokenize_lemmatize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenizes and lemmatizes text using spaCy.\n",
    "        \"\"\"\n",
    "        if (text) or (text is not None):\n",
    "            return([w.lemma_.lower() for w in self.spacy_nlp(text) if w.pos_ in ['NOUN','PROPN']])\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Fit the vectorizer on the reference text and the input texts\n",
    "        lemmatized_search_words = ' '.join(self.spacy_tokenize_lemmatize(self.search_words))\n",
    "        \n",
    "        # Fit the vectorizer on the input texts and the lemmatized reference text\n",
    "        self.vectorizer.fit(X.tolist() + [lemmatized_search_words])\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Transform the input texts to TF-IDF vectors\n",
    "        tfidf_matrix = self.vectorizer.transform(X.tolist())\n",
    "        \n",
    "        # Transform the reference text to a TF-IDF vector\n",
    "        ref_vector = self.vectorizer.transform([' '.join(self.spacy_tokenize_lemmatize(self.search_words))])\n",
    "        \n",
    "        # Calculate cosine similarity between the reference text and each input text\n",
    "        similarity_scores = cosine_similarity(ref_vector, tfidf_matrix)\n",
    "        \n",
    "        return similarity_scores.reshape(-1)\n",
    "    \n",
    "# Sample DataFrame and reference sentence\n",
    "search_words = \"Aspiring Human Resources\"\n",
    "\n",
    "# Initialize and apply the transformer\n",
    "similarity_transformer = TextSimilarityTransformerCS(search_words =search_words,  spacy_nlp=nlp)\n",
    "talents_df_clean['fit']  = similarity_transformer.fit_transform(talents_df_clean['job_title'])\n",
    "\n",
    "# Printing out the top 10 candidates based on the highest fit score\n",
    "print(talents_df_clean.sort_values(by =['fit'], ascending=False, inplace = False).head(10).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Extended N-gram model. Now let us add information from the location as well as the connections to estimate fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv('config/.env'))\n",
    "GeocodingKey= os.getenv('GeocodingKey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | job_title                                                                                                             | location                    | connection   |     fit |\n",
      "|---:|:----------------------------------------------------------------------------------------------------------------------|:----------------------------|:-------------|--------:|\n",
      "| 23 | Nortia Staffing is seeking Human Resources, Payroll & Administrative Professionals!!  (408) 709-2621                  | San Jose, California        | 500+         | 1.92337 |\n",
      "|  7 | HR Senior Specialist                                                                                                  | San Francisco Bay Area      | 500+         | 1.6     |\n",
      "| 26 | Human Resources Generalist at Schwan's                                                                                | Amerika Birleşik Devletleri | 500+         | 1.46955 |\n",
      "| 11 | Human Resources Coordinator at InterContinental Buckhead Atlanta                                                      | Atlanta, Georgia            | 500+         | 1.46955 |\n",
      "| 36 | Human Resources Management Major                                                                                      | Milpitas, California        | 18           | 1.46128 |\n",
      "| 32 | Human Resources professional for the world leader in GIS software                                                     | Highland, California        | 50           | 1.44051 |\n",
      "| 31 | HR Manager at Endemol Shine North America                                                                             | Los Angeles, California     | 268          | 1.4     |\n",
      "| 13 | Seeking Human Resources Opportunities                                                                                 | Chicago, Illinois           | 390          | 1.22337 |\n",
      "| 48 | Aspiring Human Resources Manager | Graduating May 2020 | Seeking an Entry-Level Human Resources Position in St. Louis | Cape Girardeau, Missouri    | 103          | 1.17709 |\n",
      "| 42 | Seeking Human  Resources Opportunities. Open to travel and relocation.                                                | Amerika Birleşik Devletleri | 415          | 1.16896 |\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from opencage.geocoder import OpenCageGeocode\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# This custom transformer calculates the cosine similarity after ifidf vectorizer between the job tilte and the search key terms\n",
    "class CosineSimilarityTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, search_words, vectorizer =TfidfVectorizer()) :\n",
    "        self.search_words= search_words\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure that X is treated as a Series and convert to list if necessary\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.iloc[:, 0]\n",
    "        # Fit the vectorizer on the reference sentence and the document corpus\n",
    "        self.vectorizer.fit(X.tolist() + [self.search_words])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "                # Ensure that X is treated as a Series and convert to list if necessary\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.iloc[:, 0]\n",
    "        # Transform the documents and the reference sentence into TF-IDF vectors\n",
    "        X_tfidf = self.vectorizer.transform(X.tolist())\n",
    "        sw_tfidf = self.vectorizer.transform([self.search_words])\n",
    "\n",
    "        # Calculate the cosine similarity of each document with the reference sentence\n",
    "        similarity_scores = cosine_similarity(X_tfidf, sw_tfidf)\n",
    "        \n",
    "        # Return similarity scores as a 1D array of scores\n",
    "        return similarity_scores.reshape(-1, 1)\n",
    "    \n",
    "# Usign OpenCage Geocode API to get location information\n",
    "geocoder = OpenCageGeocode(GeocodingKey)\n",
    "\n",
    "# Finding and extacting location infromation from the keywords\n",
    "def get_location_info(location_name):\n",
    "    # Assuming `geocoder` is already imported and configured\n",
    "    try:\n",
    "        results = geocoder.geocode(location_name, no_annotations='1')\n",
    "        keys = [\"city\", \"state\", \"country\"]\n",
    "        location = dict.fromkeys(keys)\n",
    "        if results:\n",
    "            # Usually, the first result is the most relevant one\n",
    "            top_result = results[0]['components']\n",
    "            for key in keys:\n",
    "                location[key] = top_result.get(key, None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error geocoding {location_name}: {e}\")\n",
    "        location = dict.fromkeys([\"city\", \"state\", \"country\"], None)\n",
    "    return location\n",
    "\n",
    "\n",
    "# Calculating the location scores, if same city, then the score is 3, if same state score is 2 and if same country then the score is 1, if none match, then zero\n",
    "def location_scores(location_names, search_words):\n",
    "\n",
    "    loc_search_kw = [ent.text for ent in nlp(search_words).ents if ent.label_ == 'GPE']\n",
    "    if loc_search_kw:\n",
    "        kw_loc_dict = get_location_info(''.join(loc_search_kw))\n",
    "        scores = []\n",
    "        for location_name in location_names:\n",
    "            location_dict = get_location_info(location_name)\n",
    "\n",
    "            score = 0\n",
    "            if loc_search_kw:\n",
    "                # This implies only one keyword is processed, which might need adjusting\n",
    "                \n",
    "                if (location_dict['country'] and (location_dict['country'] == kw_loc_dict['country'])):\n",
    "                    score += 1\n",
    "                    if (location_dict['state'] and (location_dict['state'] == kw_loc_dict['state'])):\n",
    "                        score += 1\n",
    "                        if (location_dict['city'] and (location_dict['city'] == kw_loc_dict['city'])): \n",
    "                            score += 1\n",
    "            scores.append(score)\n",
    "        return np.array(scores).reshape(-1, 1)\n",
    "    else:\n",
    "        return np.zeros(len(location_names)).reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Encoding  LinkedIn connections\n",
    "def categorize_connections(num):\n",
    "    if num =='500+ ':\n",
    "        return 3\n",
    "    elif 0 <= int(num)< 100:\n",
    "        return 0\n",
    "    elif 100 <= int(num) < 200:\n",
    "        return 1\n",
    "    elif 200 <= int(num) < 500:\n",
    "        return 2\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# Now lets run the pipeline and get the necessary info\n",
    "search_words = \"Human Resources in California\"\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "\n",
    "        ('conn', FunctionTransformer(np.vectorize(categorize_connections),  validate=False), ['connection']),\n",
    "        ('loc', FunctionTransformer(location_scores, validate=False, kw_args={'search_words': search_words}) ,  'location'),\n",
    "        ('title', CosineSimilarityTransformer(search_words = search_words, vectorizer = TfidfVectorizer(min_df=0.1, max_features=100, ngram_range=(1, 4))), ['job_title'])\n",
    "    ] \n",
    ")\n",
    "\n",
    "# Weights of each score. Let us asssign an importance score of 0.2, 0.2 and 0.6 for information contained in their no of connections, location and title respectively\n",
    "# The values of no of connections can be from 0 to 3, and no of locations also has the same range, but the cosine similarity is between -1 and 1\n",
    "weights = np.array([0.2, 0.5, 0.7 ])\n",
    "talents_df_clean['fit'] = (preprocessor.fit_transform(talents_df_clean)*weights).sum(axis=1)\n",
    "\n",
    "# Printing out the top 10 candidates based on the highest fit score\n",
    "print(talents_df_clean.sort_values(by =['fit'], ascending=False, inplace = False).head(10).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Using word embeddings word from Word2Vec and calculating the similarity score using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | job_title                                                                                            | location                    | connection   |     fit |\n",
      "|---:|:-----------------------------------------------------------------------------------------------------|:----------------------------|:-------------|--------:|\n",
      "| 23 | Nortia Staffing is seeking Human Resources, Payroll & Administrative Professionals!!  (408) 709-2621 | San Jose, California        | 500+         | 1.94212 |\n",
      "|  7 | HR Senior Specialist                                                                                 | San Francisco Bay Area      | 500+         | 1.71791 |\n",
      "| 31 | HR Manager at Endemol Shine North America                                                            | Los Angeles, California     | 268          | 1.63733 |\n",
      "| 11 | Human Resources Coordinator at InterContinental Buckhead Atlanta                                     | Atlanta, Georgia            | 500+         | 1.62132 |\n",
      "| 26 | Human Resources Generalist at Schwan's                                                               | Amerika Birleşik Devletleri | 500+         | 1.58551 |\n",
      "| 36 | Human Resources Management Major                                                                     | Milpitas, California        | 18           | 1.44777 |\n",
      "| 32 | Human Resources professional for the world leader in GIS software                                    | Highland, California        | 50           | 1.42776 |\n",
      "|  3 | People Development Coordinator at Ryan                                                               | Denton, Texas               | 500+         | 1.3641  |\n",
      "| 13 | Seeking Human Resources Opportunities                                                                | Chicago, Illinois           | 390          | 1.31057 |\n",
      "| 42 | Seeking Human  Resources Opportunities. Open to travel and relocation.                               | Amerika Birleşik Devletleri | 415          | 1.27623 |\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.utils import simple_preprocess\n",
    "import itertools\n",
    "class CosineSimilarityTransformerW2V(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, model_path, search_kw) :\n",
    "        \"\"\"\n",
    "        search_kw: str, the search keywords against which to compute similarity\n",
    "        model_path: str, path to the pre-trained word2vec model\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.search_kw = search_kw\n",
    "        self.model = None\n",
    "        self.search_words = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Load the word2vec model\n",
    "        self.model = KeyedVectors.load_word2vec_format(self.model_path, binary=True)\n",
    "        # Compute the vector for the search keywords\n",
    "        self.search_words= self.get_words(self.search_kw)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Compute cosine similarity between each sentence in X and the search keywords\n",
    "\n",
    "        return np.array([self.pairwise_cs(self.get_words(sentence), self.search_words) for _, sentence in X.items()]).reshape(-1, 1)\n",
    "    \n",
    "\n",
    "    def get_words(self, sentence):\n",
    "        # Convert sentence to tokens, ignoring out-of-vocabulary words\n",
    "        words = [word for word in simple_preprocess(sentence) if word in list(self.model.index_to_key)]\n",
    "        return words\n",
    "\n",
    "    def cosine_similarity(self, vec1, vec2):\n",
    "        # Compute cosine similarity between two vectors\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    \n",
    "    def pairwise_cs(self, sentence1, sentence2):\n",
    "        if (sentence1 and sentence2): # So that both are not empty lists\n",
    "            cosine_vals = []\n",
    "            for  word1, word2 in list(itertools.product(sentence1, sentence2)):\n",
    "                cosine_vals.append(self.cosine_similarity(self.model[word1], self.model[word2]))\n",
    "            cosine_vals = np.array(cosine_vals)\n",
    "            # Only taking the mean of the top 20 %ile of cosine values and calculating the mean as the similarity between two sentences\n",
    "            return np.mean(cosine_vals[cosine_vals >= np.percentile( cosine_vals, 80) ])\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "# Now lets run the pipeline and get the necessary info\n",
    "search_words = \"Human Resources in California\"\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "\n",
    "        ('conn', FunctionTransformer(np.vectorize(categorize_connections),  validate=False), ['connection']),\n",
    "        ('loc', FunctionTransformer(location_scores, validate=False, kw_args={'search_words': search_words}) ,  'location'),\n",
    "        ('title', CosineSimilarityTransformerW2V(search_kw = search_words, model_path = '../data/GoogleNews-vectors-negative300.bin'), 'job_title')\n",
    "    ] \n",
    ")\n",
    "\n",
    "# Weights of each score. Let us asssign an importance score of 0.2, 0.2 and 0.6 for information contained in their no of connections, location and title respectively\n",
    "# The values of no of connections can be from 0 to 3, and no of locations also has the same range, but the cosine similarity is between -1 and 1\n",
    "weights = np.array([0.2, 0.5, 0.7])\n",
    "talents_df_clean['fit'] = (preprocessor.fit_transform(talents_df_clean)*weights).sum(axis=1)\n",
    "\n",
    "# Printing out the top 10 candidates based on the highest fit score\n",
    "print(talents_df_clean.sort_values(by =['fit'], ascending=False, inplace = False).head(10).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Using sentence level representation from the [CLS] token from BERT and calculating the similarity score using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | job_title                                                                                            | location                    | connection   |     fit |\n",
      "|---:|:-----------------------------------------------------------------------------------------------------|:----------------------------|:-------------|--------:|\n",
      "|  7 | HR Senior Specialist                                                                                 | San Francisco Bay Area      | 500+         | 2.20373 |\n",
      "| 23 | Nortia Staffing is seeking Human Resources, Payroll & Administrative Professionals!!  (408) 709-2621 | San Jose, California        | 500+         | 2.17932 |\n",
      "| 31 | HR Manager at Endemol Shine North America                                                            | Los Angeles, California     | 268          | 1.94667 |\n",
      "|  3 | People Development Coordinator at Ryan                                                               | Denton, Texas               | 500+         | 1.76636 |\n",
      "| 26 | Human Resources Generalist at Schwan's                                                               | Amerika Birleşik Devletleri | 500+         | 1.72001 |\n",
      "| 11 | Human Resources Coordinator at InterContinental Buckhead Atlanta                                     | Atlanta, Georgia            | 500+         | 1.71505 |\n",
      "| 52 | Director Of Administration at Excellence Logging                                                     | Katy, Texas                 | 500+         | 1.69396 |\n",
      "|  9 | Student at Chapman University                                                                        | Lake Forest, California     | 2            | 1.63118 |\n",
      "| 41 | Admissions Representative at Community medical center long beach                                     | Long Beach, California      | 9            | 1.57903 |\n",
      "| 32 | Human Resources professional for the world leader in GIS software                                    | Highland, California        | 50           | 1.57507 |\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "class CosineSimilarityTransformerBERT(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, search_kw, model_name='bert-base-uncased') :\n",
    "        \"\"\"\n",
    "        search_kw: str, the search key words against which to compute similarity\n",
    "        model_name: str, model name of the BERT model to use\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "        self.search_kw = search_kw\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "        self.ref_vec =  None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        inputs = self.tokenizer(self.search_kw , return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            self.ref_vec = self.model(**inputs).last_hidden_state[:, 0, :].squeeze()  # CLS token representation\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Transform each sentence in the DataFrame column to cosine similarity score\n",
    "        cos_sim_scores = []\n",
    "        for  _, sentence in X.items():\n",
    "            inputs = self.tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "            with torch.no_grad():\n",
    "                sentence_vec = self.model(**inputs).last_hidden_state[:, 0, :].squeeze()\n",
    "            cos_sim = torch.nn.functional.cosine_similarity(self.ref_vec, sentence_vec, dim=0)\n",
    "            cos_sim_scores.append(cos_sim.item())\n",
    "        return np.array(cos_sim_scores).reshape(-1, 1)\n",
    "    \n",
    "\n",
    "# Now lets run the pipeline and get the necessary info\n",
    "search_words = \"Human Resources in California\"\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "\n",
    "        ('conn', FunctionTransformer(np.vectorize(categorize_connections),  validate=False), ['connection']),\n",
    "        ('loc', FunctionTransformer(location_scores, validate=False, kw_args={'search_words': search_words}) ,  'location'),\n",
    "        ('title', CosineSimilarityTransformerBERT(search_kw = search_words, model_name='bert-base-uncased'), 'job_title')\n",
    "    ] \n",
    ")\n",
    "\n",
    "# Weights of each score. Let us asssign an importance score of 0.2, 0.2 and 0.6 for information contained in their no of connections, location and title respectively\n",
    "# The values of no of connections can be from 0 to 3, and no of locations also has the same range, but the cosine similarity is between -1 and 1\n",
    "weights = np.array([0.2, 0.5, 0.7])\n",
    "talents_df_clean['fit'] = (preprocessor.fit_transform(talents_df_clean)*weights).sum(axis=1)\n",
    "\n",
    "# Printing out the top 10 candidates based on the highest fit score\n",
    "print(talents_df_clean.sort_values(by =['fit'], ascending=False, inplace = False).head(10).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Using SentenceTransformers  from Sentence-BERT and calculating the similarity score using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | job_title                                                                                            | location                    | connection   |     fit |\n",
      "|---:|:-----------------------------------------------------------------------------------------------------|:----------------------------|:-------------|--------:|\n",
      "|  7 | HR Senior Specialist                                                                                 | San Francisco Bay Area      | 500+         | 1.90726 |\n",
      "| 23 | Nortia Staffing is seeking Human Resources, Payroll & Administrative Professionals!!  (408) 709-2621 | San Jose, California        | 500+         | 1.87552 |\n",
      "| 31 | HR Manager at Endemol Shine North America                                                            | Los Angeles, California     | 268          | 1.69053 |\n",
      "| 11 | Human Resources Coordinator at InterContinental Buckhead Atlanta                                     | Atlanta, Georgia            | 500+         | 1.542   |\n",
      "| 26 | Human Resources Generalist at Schwan's                                                               | Amerika Birleşik Devletleri | 500+         | 1.45268 |\n",
      "| 36 | Human Resources Management Major                                                                     | Milpitas, California        | 18           | 1.4093  |\n",
      "|  3 | People Development Coordinator at Ryan                                                               | Denton, Texas               | 500+         | 1.38955 |\n",
      "| 32 | Human Resources professional for the world leader in GIS software                                    | Highland, California        | 50           | 1.31669 |\n",
      "| 13 | Seeking Human Resources Opportunities                                                                | Chicago, Illinois           | 390          | 1.31176 |\n",
      "| 52 | Director Of Administration at Excellence Logging                                                     | Katy, Texas                 | 500+         | 1.26844 |\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class CosineSimilarityTransformerSBERT(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self,  search_kw, model_name='all-MiniLM-L6-v2'):\n",
    "        self.model_name = model_name\n",
    "        self. search_kw =  search_kw\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.ref_vec  = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        # Precompute the search keywords embedding\n",
    "        self.ref_vec = self.model.encode(self.search_kw, convert_to_tensor=True)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Calculate cosine similarity of each sentence to the reference\n",
    "        sentence_embeddings = self.model.encode(X, convert_to_tensor=True)\n",
    "        cosine_similarities = util.pytorch_cos_sim(self.ref_vec, sentence_embeddings).flatten()\n",
    "        return cosine_similarities.numpy().reshape(-1, 1)\n",
    "# Now lets run the pipeline and get the necessary info\n",
    "\n",
    "search_words = \"Human Resources in California\"\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "\n",
    "        ('conn', FunctionTransformer(np.vectorize(categorize_connections),  validate=False), ['connection']),\n",
    "        ('loc', FunctionTransformer(location_scores, validate=False, kw_args={'search_words': search_words}) ,  'location'),\n",
    "        ('title', CosineSimilarityTransformerSBERT(search_kw = search_words, model_name='all-MiniLM-L6-v2'), 'job_title')\n",
    "    ] \n",
    ")\n",
    "\n",
    "# Weights of each score. Let us asssign an importance score of 0.2, 0.2 and 0.6 for information contained in their no of connections, location and title respectively\n",
    "# The values of no of connections can be from 0 to 3, and no of locations also has the same range, but the cosine similarity is between -1 and 1\n",
    "weights = np.array([0.2, 0.5, 0.7])\n",
    "talents_df_clean['fit'] = (preprocessor.fit_transform(talents_df_clean)*weights).sum(axis=1)\n",
    "\n",
    "# Printing out the top 10 candidates based on the highest fit score\n",
    "print(talents_df_clean.sort_values(by =['fit'], ascending=False, inplace = False).head(10).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Improving the model based on the chosen options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any profile is starred then, we can modifying the reference search vector as a weighted sum of the keywords_search vector and the starred reference vector, we can increase the cosine similarity between the new search vector and similar candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame and reference sentence\n",
    "search_words = \"Aspiring Human Resources\"\n",
    "\n",
    "# Initialize and apply the transformer\n",
    "similarity_transformer = TextSimilarityTransformer(search_words =search_words,  spacy_nlp=nlp)\n",
    "talents_df_clean['fit']  = similarity_transformer.fit_transform(talents_df_clean['job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CosineSimilarityTransformerSBERT2(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, ref_vec, model):\n",
    "        self.model = model\n",
    "        self.ref_vec  = ref_vec\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Calculate cosine similarity of each sentence to the reference\n",
    "        \n",
    "        sentence_embeddings = self.model.encode(X, convert_to_tensor=True)\n",
    "        cosine_similarities = util.pytorch_cos_sim(self.ref_vec, sentence_embeddings).flatten()\n",
    "        return cosine_similarities.numpy().reshape(-1, 1)\n",
    "    \n",
    "\n",
    "# Calculating the location scores based on the location_df, if same city, then the score is 3, if same state score is 2 and if same country then the score is 1, if none match, then zero\n",
    "def location_scores2(location_names, locations_df):\n",
    "    if locations_df.empty:\n",
    "        scores = []\n",
    "        for location_name in location_names:\n",
    "            location_dict = get_location_info(location_name)\n",
    "            score = 0\n",
    "            if location_dict:\n",
    "                for i, loc_row in locations_df.iterrows():\n",
    "                    if (location_dict['country'] and (location_dict['country'] == loc_row['country'])):\n",
    "                        score += 1\n",
    "                        if (location_dict['state'] and (location_dict['state'] == loc_row['state'])):\n",
    "                            score += 1\n",
    "                            if (location_dict['city'] and (location_dict['city'] == loc_row['city'])): \n",
    "                                score += 1\n",
    "            scores.append(score)\n",
    "        scores = np.array(scores)\n",
    "        scaled_scores = (scores - scores.min(axis=0)) / (scores.max(axis =0) - scores.min(axis=0)) \n",
    "        # Scaling scores so that they are in a range between 0 and 1\n",
    "        return scaled_scores.reshape(-1, 1)\n",
    "    else:\n",
    "        return np.zeros(len(location_names)).reshape(-1, 1)\n",
    "\n",
    "# Encoding  LinkedIn connections so that they are always between 0 and 1\n",
    "def categorize_connections(num, threshold = 0):\n",
    "    if num:\n",
    "        if not threshold:\n",
    "            if num =='500+ ':\n",
    "                return 3/3\n",
    "            elif 0 <= int(num)< 100:\n",
    "                return 0/3\n",
    "            elif 100 <= int(num) < 200:\n",
    "                return 1/3\n",
    "            elif 200 <= int(num) < 500:\n",
    "                return 2/3\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            if num>= threshold:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "search_words = \"Human Resources in California\"\n",
    "locations_df = pd.DataFrame(columns = ['country','state', 'city'])\n",
    "search_loc = [ent.text for ent in nlp(search_words).ents if ent.label_ == 'GPE']\n",
    "if search_loc:\n",
    "    search_loc_dict = get_location_info(' '.join(search_loc))\n",
    "    locations_df = locations_df.append(search_loc_dict,  ignore_index=True)\n",
    "\n",
    "thresholds = [0]\n",
    "sindx = 10\n",
    "\n",
    "\n",
    "# Extracting information from the job title to update the search _vector\n",
    "\n",
    "job_title_col = talents_df_clean.loc[sindx, 'job_title']\n",
    "\n",
    "\n",
    "# Extracting information for updating locations\n",
    "location_col = talents_df_clean.loc[sindx, 'location']\n",
    "locations_df = locations_df.append(get_location_info(location_col),  ignore_index=True)\n",
    "\n",
    "# Extracting information for updating connections\n",
    "\n",
    "connections_col = talents_df_clean.loc[sindx, 'connection']\n",
    "if connections_col=='500+':\n",
    "    thresholds.append(500)\n",
    "else:\n",
    "    thresholds.append(int(connections_col))\n",
    "\n",
    "threshold = np.mean(thresholds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessor_init = ColumnTransformer(\n",
    "    transformers=[\n",
    "\n",
    "        ('conn', FunctionTransformer(np.vectorize(categorize_connections),  validate=False), ['connection']),\n",
    "        ('loc', FunctionTransformer(location_scores, validate=False, kw_args={'search_words': search_words}) ,  'location'),\n",
    "        ('title', CosineSimilarityTransformerSBERTU(search_kw = search_words, model_name='all-MiniLM-L6-v2'), 'job_title')\n",
    "    ] \n",
    ")\n",
    "\n",
    "        self. search_kw =  search_kw\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "transformed_df = transformers.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
