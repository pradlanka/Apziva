{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Project 3: Potential Talents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background:\n",
    "\n",
    "As a talent sourcing and management company, we are interested in finding talented individuals for sourcing these candidates to technology companies. Finding talented candidates is not easy, for several reasons. The first reason is one needs to understand what the role is very well to fill in that spot, this requires understanding the client’s needs and what they are looking for in a potential candidate. The second reason is one needs to understand what makes a candidate shine for the role we are in search for. Third, where to find talented individuals is another challenge.\n",
    "\n",
    "The nature of our job requires a lot of human labor and is full of manual operations. Towards automating this process we want to build a better approach that could save us time and finally help us spot potential candidates that could fit the roles we are in search for. Moreover, going beyond that for a specific role we want to fill in we are interested in developing a machine learning powered pipeline that could spot talented individuals, and rank them based on their fitness.\n",
    "\n",
    "We are right now semi-automatically sourcing a few candidates, therefore the sourcing part is not a concern at this time but we expect to first determine best matching candidates based on how fit these candidates are for a given role. We generally make these searches based on some keywords such as “full-stack software engineer”, “engineering manager” or “aspiring human resources” based on the role we are trying to fill in. These keywords might change, and you can expect that specific keywords will be provided to you.\n",
    "\n",
    "Assuming that we were able to list and rank fitting candidates, we then employ a review procedure, as each candidate needs to be reviewed and then determined how good a fit they are through manual inspection. This procedure is done manually and at the end of this manual review, we might choose not the first fitting candidate in the list but maybe the 7th candidate in the list. If that happens, we are interested in being able to re-rank the previous list based on this information. This supervisory signal is going to be supplied by starring the 7th candidate in the list. Starring one candidate actually sets this candidate as an ideal candidate for the given role. Then, we expect the list to be re-ranked each time a candidate is starred.\n",
    "\n",
    "Data Description:\n",
    "\n",
    "The data comes from our sourcing efforts. We removed any field that could directly reveal personal details and gave a unique identifier for each candidate.\n",
    "\n",
    "Attributes:\n",
    "id : unique identifier for candidate (numeric)\n",
    "\n",
    "job_title : job title for candidate (text)\n",
    "\n",
    "location : geographical location for candidate (text)\n",
    "\n",
    "connections: number of connections candidate has, 500+ means over 500 (text)\n",
    "\n",
    "Output (desired target):\n",
    "fit - how fit the candidate is for the role? (numeric, probability between 0-1)\n",
    "\n",
    "Keywords: “Aspiring human resources” or “seeking human resources”\n",
    "\n",
    "Download Data:\n",
    "\n",
    "https://docs.google.com/spreadsheets/d/117X6i53dKiO7w6kuA1g1TpdTlv1173h_dPlJt5cNNMU/edit?usp=sharing\n",
    "\n",
    "Goal(s):\n",
    "\n",
    "Predict how fit the candidate is based on their available information (variable fit)\n",
    "\n",
    "Success Metric(s):\n",
    "\n",
    "Rank candidates based on a fitness score.\n",
    "\n",
    "Re-rank candidates when a candidate is starred.\n",
    "\n",
    "Bonus(es):\n",
    "\n",
    "We are interested in a robust algorithm, tell us how your solution works and show us how your ranking gets better with each starring action.\n",
    "\n",
    "How can we filter out candidates which in the first place should not be in this list?\n",
    "\n",
    "Can we determine a cut-off point that would work for other roles without losing high potential candidates?\n",
    "\n",
    "Do you have any ideas that we should explore so that we can even automate this procedure to prevent human bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the necessary packages and the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could use spaCy to tokenize and stemm our keywords for NLP analyses\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2019 C.T. Bauer College of Business Graduate (...</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>85</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Native English Teacher at EPIK (English Progra...</td>\n",
       "      <td>Kanada</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>People Development Coordinator at Ryan</td>\n",
       "      <td>Denton, Texas</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Advisory Board Member at Celal Bayar University</td>\n",
       "      <td>İzmir, Türkiye</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Aspiring Human Resources Specialist</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Student at Humber College and Aspiring Human R...</td>\n",
       "      <td>Kanada</td>\n",
       "      <td>61</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>HR Senior Specialist</td>\n",
       "      <td>San Francisco Bay Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Student at Humber College and Aspiring Human R...</td>\n",
       "      <td>Kanada</td>\n",
       "      <td>61</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Seeking Human Resources HRIS and Generalist Po...</td>\n",
       "      <td>Greater Philadelphia Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          job_title  \\\n",
       "0   1  2019 C.T. Bauer College of Business Graduate (...   \n",
       "1   2  Native English Teacher at EPIK (English Progra...   \n",
       "2   3              Aspiring Human Resources Professional   \n",
       "3   4             People Development Coordinator at Ryan   \n",
       "4   5    Advisory Board Member at Celal Bayar University   \n",
       "5   6                Aspiring Human Resources Specialist   \n",
       "6   7  Student at Humber College and Aspiring Human R...   \n",
       "7   8                               HR Senior Specialist   \n",
       "8   9  Student at Humber College and Aspiring Human R...   \n",
       "9  10  Seeking Human Resources HRIS and Generalist Po...   \n",
       "\n",
       "                              location connection  fit  \n",
       "0                       Houston, Texas         85  NaN  \n",
       "1                               Kanada      500+   NaN  \n",
       "2  Raleigh-Durham, North Carolina Area         44  NaN  \n",
       "3                        Denton, Texas      500+   NaN  \n",
       "4                       İzmir, Türkiye      500+   NaN  \n",
       "5           Greater New York City Area          1  NaN  \n",
       "6                               Kanada         61  NaN  \n",
       "7               San Francisco Bay Area      500+   NaN  \n",
       "8                               Kanada         61  NaN  \n",
       "9            Greater Philadelphia Area      500+   NaN  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the raw data\n",
    "talents_df =  pd.read_excel('potential-talents.xlsx')\n",
    "print(talents_df.shape)\n",
    "talents_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data frame  it is clear that there area few duplicates. Lets remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019 C.T. Bauer College of Business Graduate (...</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>85</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Native English Teacher at EPIK (English Progra...</td>\n",
       "      <td>Kanada</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>People Development Coordinator at Ryan</td>\n",
       "      <td>Denton, Texas</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Advisory Board Member at Celal Bayar University</td>\n",
       "      <td>İzmir, Türkiye</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Aspiring Human Resources Specialist</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Student at Humber College and Aspiring Human R...</td>\n",
       "      <td>Kanada</td>\n",
       "      <td>61</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HR Senior Specialist</td>\n",
       "      <td>San Francisco Bay Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Seeking Human Resources HRIS and Generalist Po...</td>\n",
       "      <td>Greater Philadelphia Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Student at Chapman University</td>\n",
       "      <td>Lake Forest, California</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_title  \\\n",
       "0  2019 C.T. Bauer College of Business Graduate (...   \n",
       "1  Native English Teacher at EPIK (English Progra...   \n",
       "2              Aspiring Human Resources Professional   \n",
       "3             People Development Coordinator at Ryan   \n",
       "4    Advisory Board Member at Celal Bayar University   \n",
       "5                Aspiring Human Resources Specialist   \n",
       "6  Student at Humber College and Aspiring Human R...   \n",
       "7                               HR Senior Specialist   \n",
       "8  Seeking Human Resources HRIS and Generalist Po...   \n",
       "9                      Student at Chapman University   \n",
       "\n",
       "                              location connection  fit  \n",
       "0                       Houston, Texas         85  NaN  \n",
       "1                               Kanada      500+   NaN  \n",
       "2  Raleigh-Durham, North Carolina Area         44  NaN  \n",
       "3                        Denton, Texas      500+   NaN  \n",
       "4                       İzmir, Türkiye      500+   NaN  \n",
       "5           Greater New York City Area          1  NaN  \n",
       "6                               Kanada         61  NaN  \n",
       "7               San Francisco Bay Area      500+   NaN  \n",
       "8            Greater Philadelphia Area      500+   NaN  \n",
       "9              Lake Forest, California          2  NaN  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the duplicates in the data\n",
    "talents_df_clean = talents_df.drop(columns= 'id').drop_duplicates().reset_index(drop= True)\n",
    "print(talents_df_clean.shape)\n",
    "talents_df_clean.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are just 53 unique rows rather than 104 entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Modeling\n",
    "\n",
    "We can estimate the fit score by looking for matching or similar words between the search term and candidate job title extracted from this profile.  We can additionally use location information and connections to improve the ranking. Here we implement this using spacy to tokenize and stem the words, then we can use the tf-idf vectorizer to vectorize our words and use either jacquard similarity or cosine transformation to find the similarity between the search term and candidate job title.\n",
    "\n",
    "Furthermore we can hard-code the importance of the no of connections by encoding them as ordingal classes with\n",
    "\n",
    "<100 :0 (low); 101-200: 1 (medium); 200-500: 2 (high); 500+: 3 (very-high)\n",
    "\n",
    "\n",
    "As for location, we can extract the city, state and country information from the 'location' column and match if it is the same location as the job search keyword and assign a score accordingly.\n",
    "so if just the country matches a score of 1 is matched, if additionaly the state matches a score of 2 is assigned and finally if city also matches then a score of 3 is assigned. If the country is not matched, then a score of 0 is assigned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0 Just looking at similar/same words  between the \"search term keywords\" and the \"job profile\"  and quanitfying similarity through jacquard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Kokomo, Indiana Area</td>\n",
       "      <td>71</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Seeking Human Resources Opportunities</td>\n",
       "      <td>Chicago, Illinois</td>\n",
       "      <td>390</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Aspiring Human Resources Specialist</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Seeking Human  Resources Opportunities. Open t...</td>\n",
       "      <td>Amerika Birleşik Devletleri</td>\n",
       "      <td>415</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Student at Humber College and Aspiring Human R...</td>\n",
       "      <td>Kanada</td>\n",
       "      <td>61</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Business Management Major and Aspiring Human R...</td>\n",
       "      <td>Monroe, Louisiana Area</td>\n",
       "      <td>5</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Seeking Human Resources Position</td>\n",
       "      <td>Las Vegas, Nevada Area</td>\n",
       "      <td>48</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Human Resources Professional</td>\n",
       "      <td>Greater Boston Area</td>\n",
       "      <td>16</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Human Resources Management Major</td>\n",
       "      <td>Milpitas, California</td>\n",
       "      <td>18</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            job_title  \\\n",
       "45              Aspiring Human Resources Professional   \n",
       "2               Aspiring Human Resources Professional   \n",
       "13              Seeking Human Resources Opportunities   \n",
       "5                 Aspiring Human Resources Specialist   \n",
       "42  Seeking Human  Resources Opportunities. Open t...   \n",
       "6   Student at Humber College and Aspiring Human R...   \n",
       "20  Business Management Major and Aspiring Human R...   \n",
       "47                   Seeking Human Resources Position   \n",
       "22                       Human Resources Professional   \n",
       "36                   Human Resources Management Major   \n",
       "\n",
       "                               location connection       fit  \n",
       "45                 Kokomo, Indiana Area         71  0.666667  \n",
       "2   Raleigh-Durham, North Carolina Area         44  0.666667  \n",
       "13                    Chicago, Illinois        390  0.666667  \n",
       "5            Greater New York City Area          1  0.666667  \n",
       "42          Amerika Birleşik Devletleri        415  0.400000  \n",
       "6                                Kanada         61  0.333333  \n",
       "20               Monroe, Louisiana Area          5  0.333333  \n",
       "47               Las Vegas, Nevada Area         48  0.250000  \n",
       "22                  Greater Boston Area         16  0.250000  \n",
       "36                 Milpitas, California         18  0.200000  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextSimilarityTransformerJS(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, search_words, spacy_nlp):\n",
    "        \"\"\"\n",
    "        Custom transformer to calculate jacquard similarity  \n",
    "        between the search key words and the job_titles.\n",
    "        \"\"\"\n",
    "        self.search_words = search_words\n",
    "        self.spacy_nlp = spacy_nlp\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def transform(self, X):\n",
    "      \n",
    "        # Calculate jacquared similarity between the job search words and job title of the candidates. \n",
    "        similarity_scores = X.apply(self.job_similarity_kw_js, search_words= self.search_words)\n",
    "    \n",
    "        return similarity_scores\n",
    "    \n",
    "\n",
    "    def spacy_tokenize_lemmatize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenizes and lemmatizes text using spaCy.\n",
    "        \"\"\"\n",
    "        if (text) or (text is not None):\n",
    "            # We only want to use nouns and proper nouns as the filler words \n",
    "            # do not add much to improve the score\n",
    "            return([w.lemma_.lower() for w in self.spacy_nlp(text) if w.pos_ in ['NOUN','PROPN']])\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    # Defining jacquard similarity\n",
    "    def job_similarity_kw_js(self, job_title, search_words):\n",
    "\n",
    "        \"\"\"\n",
    "        Calculates jacqurd similarity between 2 sentences/phrases\n",
    "        \"\"\"\n",
    "        job_title_token = set(self.spacy_tokenize_lemmatize(job_title))\n",
    "        search_words_token = set(self.spacy_tokenize_lemmatize(search_words))\n",
    "        n_common_words = len(job_title_token.intersection(search_words_token))\n",
    "        total_words = len(job_title_token.union(search_words_token))\n",
    "        return n_common_words/total_words\n",
    "\n",
    "    \n",
    "\n",
    "# Sample search keywrods that we use to find the fit\n",
    "search_words = \"Aspiring Human Resources\"\n",
    "\n",
    "# Initialize and apply the transformer\n",
    "similarity_transformer = TextSimilarityTransformerJS(search_words =search_words,  spacy_nlp=nlp)\n",
    "talents_df_clean['fit']  = similarity_transformer.fit_transform(talents_df_clean['job_title'])\n",
    "\n",
    "#Displaying candidates in descending order of fit score\n",
    "talents_df_clean.sort_values(by =['fit'], ascending=False, inplace = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this model is that we are looking at similar words across the job title and keywords, so even though HR and Human resources mean the same this model thinks of them as different tokens. Also another major issue with this model is that this model does not consider the order of the words, for eg. for this model, resources human is same as human resources. This issue can be adressed by considering the n-gram models discussed next. Finally, this model penalizes job titles that have useful information but just dont have same words as the titles, i.e, this model would give a score of 1 to job titles that are same as the search terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Using N-Gram models and Tf-Idf Vectorizers and using cosine transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Human Resources|\\nConflict Management|\\nPolici...</td>\n",
       "      <td>Dallas/Fort Worth Area</td>\n",
       "      <td>409</td>\n",
       "      <td>0.569669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Human Resources Management Major</td>\n",
       "      <td>Milpitas, California</td>\n",
       "      <td>18</td>\n",
       "      <td>0.569669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Director of Human Resources North America, Gro...</td>\n",
       "      <td>Greater Grand Rapids, Michigan Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.569669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Human Resources Generalist at Schwan's</td>\n",
       "      <td>Amerika Birleşik Devletleri</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.458555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Human Resources Coordinator at InterContinenta...</td>\n",
       "      <td>Atlanta, Georgia</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.458555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Human Resources Generalist at ScottMadden, Inc.</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.458555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Human Resources Specialist at Luxottica</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.458555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Senior Human Resources Business Partner at Hei...</td>\n",
       "      <td>Chattanooga, Tennessee Area</td>\n",
       "      <td>455</td>\n",
       "      <td>0.458555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Director Human Resources  at EY</td>\n",
       "      <td>Greater Atlanta Area</td>\n",
       "      <td>349</td>\n",
       "      <td>0.458555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Human Resources Generalist at Loparex</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.458555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            job_title  \\\n",
       "25  Human Resources|\\nConflict Management|\\nPolici...   \n",
       "36                   Human Resources Management Major   \n",
       "17  Director of Human Resources North America, Gro...   \n",
       "26             Human Resources Generalist at Schwan's   \n",
       "11  Human Resources Coordinator at InterContinenta...   \n",
       "19    Human Resources Generalist at ScottMadden, Inc.   \n",
       "16            Human Resources Specialist at Luxottica   \n",
       "29  Senior Human Resources Business Partner at Hei...   \n",
       "37                    Director Human Resources  at EY   \n",
       "49              Human Resources Generalist at Loparex   \n",
       "\n",
       "                               location connection       fit  \n",
       "25               Dallas/Fort Worth Area        409  0.569669  \n",
       "36                 Milpitas, California         18  0.569669  \n",
       "17  Greater Grand Rapids, Michigan Area      500+   0.569669  \n",
       "26          Amerika Birleşik Devletleri      500+   0.458555  \n",
       "11                     Atlanta, Georgia      500+   0.458555  \n",
       "19  Raleigh-Durham, North Carolina Area      500+   0.458555  \n",
       "16           Greater New York City Area      500+   0.458555  \n",
       "29          Chattanooga, Tennessee Area        455  0.458555  \n",
       "37                 Greater Atlanta Area        349  0.458555  \n",
       "49  Raleigh-Durham, North Carolina Area      500+   0.458555  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "    \n",
    "class TextSimilarityTransformerCS(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, search_words, spacy_nlp):\n",
    "        \"\"\"\n",
    "        Custom transformer to calculate cosine similarity \n",
    "        between the search key words and the job_titles after td-idf vectorizer.\n",
    "\n",
    "        \"\"\"\n",
    "        self.search_words = search_words\n",
    "        self.spacy_nlp = spacy_nlp\n",
    "        stop_words_tokenized = set(self.spacy_tokenize_lemmatize(' '.join(sorted(STOP_WORDS))))\n",
    "        self.vectorizer =  TfidfVectorizer(min_df =0.1, max_features=100, \n",
    "                                    stop_words= list(stop_words_tokenized), ngram_range = (1,4))\n",
    "        \n",
    "    \n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Fit the vectorizer on the search keywords and the job titles\n",
    "        lemmatized_search_words = ' '.join(self.spacy_tokenize_lemmatize(self.search_words))\n",
    "        \n",
    "        # Fit the vectorizer on the input texts and the lemmatized reference text\n",
    "        self.vectorizer.fit(X.tolist() + [lemmatized_search_words])\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Transform the job titles to TF-IDF vectors\n",
    "        tfidf_matrix = self.vectorizer.transform(X.tolist())\n",
    "        \n",
    "        # Transform the search key words to a TF-IDF vector\n",
    "        ref_vector = self.vectorizer.transform([' '.join(self.spacy_tokenize_lemmatize(self.search_words))])\n",
    "        \n",
    "        # Calculate cosine similarity between search keywrord vectors and job title vectors\n",
    "        similarity_scores = cosine_similarity(ref_vector, tfidf_matrix)\n",
    "        \n",
    "        return similarity_scores.reshape(-1)\n",
    "    \n",
    "    \n",
    "    def spacy_tokenize_lemmatize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenizes and lemmatizes text using spaCy.\n",
    "        \"\"\"\n",
    "        if (text) or (text is not None):\n",
    "            # Only using proper nouns and nouns\n",
    "            return([w.lemma_.lower() for w in self.spacy_nlp(text) if w.pos_ in ['NOUN','PROPN']])\n",
    "    \n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "# Sample search keywrods that we use to find the fit\n",
    "search_words = \"Aspiring Human Resources\"\n",
    "\n",
    "# Initialize and apply the transformer\n",
    "\n",
    "similarity_transformer = TextSimilarityTransformerCS(search_words =search_words,  spacy_nlp=nlp)\n",
    "talents_df_clean['fit']  = similarity_transformer.fit_transform(talents_df_clean['job_title'])\n",
    "\n",
    "#Displaying candidates in descending order of fit score\n",
    "talents_df_clean.sort_values(by =['fit'], ascending=False, inplace = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said earlier, here we are using an n-gram model, with 1-gram, 2-gram 3 and 4-gram tokens. So word order is preserved. However, this model still relies on counts of occurences of tokens between the search word and job titles. So similar words are still not addressed, which word2vec model address as discussed later. Secondly, idf transformer in the tf-idf vecotrizer penalizes common words across the document, which in this case unfortunaley penalizes the term \"Human resources\" because it occurs across multiple job titles, and boosts rarer words which unfortunately may not be directy linked to the job term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Extended N-gram model. Now let us add information from the location as well as the connections to estimate fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only used information in the job titles to estimate the fit score. We can also use the location informtion and the connection scores to get more relavent candidates by improving the fit scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using OpenCage Geocode API \n",
    "# Also extracting the word2vec model path\n",
    "# Loading the API Key from the envir. variables\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv('config/.env'))\n",
    "GeocodingKey= os.getenv('GeocodingKey')\n",
    "WORD2VEC_PATH= os.getenv('GoogleNewsPath')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Nortia Staffing is seeking Human Resources, Pa...</td>\n",
       "      <td>San Jose, California</td>\n",
       "      <td>500+</td>\n",
       "      <td>1.923369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HR Senior Specialist</td>\n",
       "      <td>San Francisco Bay Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>1.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Human Resources Generalist at Schwan's</td>\n",
       "      <td>Amerika Birleşik Devletleri</td>\n",
       "      <td>500+</td>\n",
       "      <td>1.469546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Human Resources Coordinator at InterContinenta...</td>\n",
       "      <td>Atlanta, Georgia</td>\n",
       "      <td>500+</td>\n",
       "      <td>1.469546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Human Resources Management Major</td>\n",
       "      <td>Milpitas, California</td>\n",
       "      <td>18</td>\n",
       "      <td>1.461281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Human Resources professional for the world lea...</td>\n",
       "      <td>Highland, California</td>\n",
       "      <td>50</td>\n",
       "      <td>1.440512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>HR Manager at Endemol Shine North America</td>\n",
       "      <td>Los Angeles, California</td>\n",
       "      <td>268</td>\n",
       "      <td>1.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Seeking Human Resources Opportunities</td>\n",
       "      <td>Chicago, Illinois</td>\n",
       "      <td>390</td>\n",
       "      <td>1.223369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Aspiring Human Resources Manager | Graduating ...</td>\n",
       "      <td>Cape Girardeau, Missouri</td>\n",
       "      <td>103</td>\n",
       "      <td>1.177091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Seeking Human  Resources Opportunities. Open t...</td>\n",
       "      <td>Amerika Birleşik Devletleri</td>\n",
       "      <td>415</td>\n",
       "      <td>1.168958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            job_title  \\\n",
       "23  Nortia Staffing is seeking Human Resources, Pa...   \n",
       "7                                HR Senior Specialist   \n",
       "26             Human Resources Generalist at Schwan's   \n",
       "11  Human Resources Coordinator at InterContinenta...   \n",
       "36                   Human Resources Management Major   \n",
       "32  Human Resources professional for the world lea...   \n",
       "31          HR Manager at Endemol Shine North America   \n",
       "13              Seeking Human Resources Opportunities   \n",
       "48  Aspiring Human Resources Manager | Graduating ...   \n",
       "42  Seeking Human  Resources Opportunities. Open t...   \n",
       "\n",
       "                       location connection       fit  \n",
       "23         San Jose, California      500+   1.923369  \n",
       "7        San Francisco Bay Area      500+   1.600000  \n",
       "26  Amerika Birleşik Devletleri      500+   1.469546  \n",
       "11             Atlanta, Georgia      500+   1.469546  \n",
       "36         Milpitas, California         18  1.461281  \n",
       "32         Highland, California         50  1.440512  \n",
       "31      Los Angeles, California        268  1.400000  \n",
       "13            Chicago, Illinois        390  1.223369  \n",
       "48     Cape Girardeau, Missouri        103  1.177091  \n",
       "42  Amerika Birleşik Devletleri        415  1.168958  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from opencage.geocoder import OpenCageGeocode\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# This custom transformer calculates the cosine similarity after tfidf vectorizer \n",
    "# between the job tilte and the search key terms\n",
    "class CosineSimilarityTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, search_words, vectorizer =TfidfVectorizer()) :\n",
    "        \n",
    "        \"\"\"\n",
    "        Custom transformer to calculate cosine similarity \n",
    "        between the search key words and the job_titles after td-idf vectorizer.\n",
    "        \n",
    "        search_words: str, the search keywords against which to compute similarity\n",
    "        vectorizer: Vectorizer deafualts ot tfidf vectorizer to convert words to vectors\n",
    "\n",
    "        \"\"\"\n",
    "        self.search_words= search_words\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure that X is treated as a Series and convert to list if necessary\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.iloc[:, 0]\n",
    "        # Fit the vectorizer on the search keywords  and the job titles\n",
    "        self.vectorizer.fit(X.tolist() + [self.search_words])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "                # Ensure that X is treated as a Series and convert to list if necessary\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.iloc[:, 0]\n",
    "        # Transform the search keywords and the job titles into TF-IDF vectors\n",
    "        X_tfidf = self.vectorizer.transform(X.tolist())\n",
    "        sw_tfidf = self.vectorizer.transform([self.search_words])\n",
    "\n",
    "        # Calculate the cosine similarity of job titles with the search keywords\n",
    "        similarity_scores = cosine_similarity(X_tfidf, sw_tfidf)\n",
    "        \n",
    "        # Return similarity scores as a 1D array of scores\n",
    "        return similarity_scores.reshape(-1, 1)\n",
    "    \n",
    "# Usign OpenCage Geocode API to get location information and configuring it\n",
    "geocoder = OpenCageGeocode(GeocodingKey)\n",
    "\n",
    "# Finding and extacting location infromation from the keywords\n",
    "def get_location_info(location_name):\n",
    "   \n",
    "    \"\"\"\n",
    "    uses location name string and outputs the city state and country information for the top search result \n",
    "    for the location string as a dict after quertying the Open Cage Geocode API\n",
    "\n",
    "\n",
    "\n",
    "    Args:\n",
    "        location names : string containing location information\n",
    "\n",
    "    Returns:\n",
    "        dict: TDictionary containg the city, state and country inforation for the search string\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "    # Querying the API for location info\n",
    "        results = geocoder.geocode(location_name, no_annotations='1')\n",
    "        keys = [\"city\", \"state\", \"country\"]\n",
    "        location = dict.fromkeys(keys)\n",
    "        if results:\n",
    "            # Usually, the first result is the most relevant one\n",
    "            top_result = results[0]['components']\n",
    "            for key in keys:\n",
    "                location[key] = top_result.get(key, None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error geocoding {location_name}: {e}\")\n",
    "        location = dict.fromkeys([\"city\", \"state\", \"country\"], None)\n",
    "    return location\n",
    "\n",
    "\n",
    "# Calculating the location scores, if same city, then the score is 3, if same state score is 2 and \n",
    "# if same country then the score is 1, if none match, then zero\n",
    "def location_scores(location_names, search_words):\n",
    "    \"\"\"\n",
    "    uses  search keywords and information in the location column of a data frame to find similarity\n",
    " \n",
    "    Args:\n",
    "        location names : Data frame column  (pandas series) containing locatoin information for job candidates\n",
    "        search_words :  str containing the search term\n",
    "\n",
    "    Returns:\n",
    "        pd.series: The calculated similarity scores between loc column and search string\n",
    "    \"\"\"\n",
    "    loc_search_kw = [ent.text for ent in nlp(search_words).ents if ent.label_ == 'GPE']\n",
    "    if loc_search_kw:\n",
    "        kw_loc_dict = get_location_info(''.join(loc_search_kw))\n",
    "        scores = []\n",
    "        for location_name in location_names:\n",
    "            location_dict = get_location_info(location_name)\n",
    "\n",
    "            score = 0\n",
    "            if loc_search_kw:\n",
    "                \n",
    "                if (location_dict['country'] and (location_dict['country'] == kw_loc_dict['country'])):\n",
    "                    score += 1\n",
    "                    if (location_dict['state'] and (location_dict['state'] == kw_loc_dict['state'])):\n",
    "                        score += 1\n",
    "                        if (location_dict['city'] and (location_dict['city'] == kw_loc_dict['city'])): \n",
    "                            score += 1\n",
    "            scores.append(score)\n",
    "        return np.array(scores).reshape(-1, 1)\n",
    "    else:\n",
    "        return np.zeros(len(location_names)).reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Encoding  LinkedIn connections\n",
    "def categorize_connections(num:int) -> int:\n",
    "\n",
    "    '''\n",
    "    Bins the no of LinkedIn connections to ordinal values\n",
    "    '''\n",
    "    if num =='500+ ':\n",
    "        return 3\n",
    "    elif 0 <= int(num)< 100:\n",
    "        return 0\n",
    "    elif 100 <= int(num) < 200:\n",
    "        return 1\n",
    "    elif 200 <= int(num) < 500:\n",
    "        return 2\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# run the pipeline and and get the fit score \n",
    "\n",
    "search_words = \"Human Resources in California\" # location information in the search words can improve predictions\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "\n",
    "        ('conn', FunctionTransformer(np.vectorize(categorize_connections),  validate=False), ['connection']),\n",
    "        ('loc', FunctionTransformer(location_scores, validate=False, kw_args={'search_words': search_words}) ,  'location'),\n",
    "        ('title', CosineSimilarityTransformer(search_words = search_words, \n",
    "                                              vectorizer = TfidfVectorizer(min_df=0.1, max_features=100, ngram_range=(1, 4))), ['job_title'])\n",
    "    ] \n",
    ")\n",
    "\n",
    "# Weights of each score. Let us asssign an importance score of 0.2, 0.5 and 0.7 for information \n",
    "# contained in their no of connections, location and title respectively\n",
    "# The values of no of connections can be from 0 to 3, and no of locations also has the same range, \n",
    "# but the cosine similarity is between -1 and 1\n",
    "\n",
    "weights = np.array([0.2, 0.5, 0.7 ]) \n",
    "talents_df_clean['fit'] = (preprocessor.fit_transform(talents_df_clean)*weights).sum(axis=1)\n",
    "\n",
    "#Displaying candidates in descending order of fit score\n",
    "talents_df_clean.sort_values(by =['fit'], ascending=False, inplace = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding the location infomation, a lot of top search results are now from California, thus improving our fit score estimation. Similarly, a lot of the top results have location have a high number of connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Using word embeddings word from Word2Vec utlizing Gensim module and calculating the similarity score between pairwise word embeddings using cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use break the sentence into individual words and use the word embeddings from word2vec models to capture similarity using cosine transformation. Just averaging the word vectors from a sentence may dilute its meaning, hence we will calculate the pairwise cosine simiarity of vectors from words from the job tiles and the search word and just take the mean of the top 20%ile of the pairwise cosine values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Nortia Staffing is seeking Human Resources, Pa...</td>\n",
       "      <td>San Jose, California</td>\n",
       "      <td>500+</td>\n",
       "      <td>1.942121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HR Senior Specialist</td>\n",
       "      <td>San Francisco Bay Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>1.717908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>HR Manager at Endemol Shine North America</td>\n",
       "      <td>Los Angeles, California</td>\n",
       "      <td>268</td>\n",
       "      <td>1.637331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Human Resources Coordinator at InterContinenta...</td>\n",
       "      <td>Atlanta, Georgia</td>\n",
       "      <td>500+</td>\n",
       "      <td>1.621325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Human Resources Generalist at Schwan's</td>\n",
       "      <td>Amerika Birleşik Devletleri</td>\n",
       "      <td>500+</td>\n",
       "      <td>1.585514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Human Resources Management Major</td>\n",
       "      <td>Milpitas, California</td>\n",
       "      <td>18</td>\n",
       "      <td>1.447768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Human Resources professional for the world lea...</td>\n",
       "      <td>Highland, California</td>\n",
       "      <td>50</td>\n",
       "      <td>1.427759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>People Development Coordinator at Ryan</td>\n",
       "      <td>Denton, Texas</td>\n",
       "      <td>500+</td>\n",
       "      <td>1.364098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Seeking Human Resources Opportunities</td>\n",
       "      <td>Chicago, Illinois</td>\n",
       "      <td>390</td>\n",
       "      <td>1.310571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Seeking Human  Resources Opportunities. Open t...</td>\n",
       "      <td>Amerika Birleşik Devletleri</td>\n",
       "      <td>415</td>\n",
       "      <td>1.276230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            job_title  \\\n",
       "23  Nortia Staffing is seeking Human Resources, Pa...   \n",
       "7                                HR Senior Specialist   \n",
       "31          HR Manager at Endemol Shine North America   \n",
       "11  Human Resources Coordinator at InterContinenta...   \n",
       "26             Human Resources Generalist at Schwan's   \n",
       "36                   Human Resources Management Major   \n",
       "32  Human Resources professional for the world lea...   \n",
       "3              People Development Coordinator at Ryan   \n",
       "13              Seeking Human Resources Opportunities   \n",
       "42  Seeking Human  Resources Opportunities. Open t...   \n",
       "\n",
       "                       location connection       fit  \n",
       "23         San Jose, California      500+   1.942121  \n",
       "7        San Francisco Bay Area      500+   1.717908  \n",
       "31      Los Angeles, California        268  1.637331  \n",
       "11             Atlanta, Georgia      500+   1.621325  \n",
       "26  Amerika Birleşik Devletleri      500+   1.585514  \n",
       "36         Milpitas, California         18  1.447768  \n",
       "32         Highland, California         50  1.427759  \n",
       "3                 Denton, Texas      500+   1.364098  \n",
       "13            Chicago, Illinois        390  1.310571  \n",
       "42  Amerika Birleşik Devletleri        415  1.276230  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.utils import simple_preprocess\n",
    "import itertools\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "class CosineSimilarityTransformerW2V(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, model_path, search_kw) :\n",
    "        \"\"\"\n",
    "        Custom transformer to calculate mean pairwise cosine similarity \n",
    "        between the words in the search key words and the job_titles after calculating\n",
    "        word embeddings\n",
    "\n",
    "        search_kw: str, the search keywords against which to compute similarity\n",
    "        model_path: str, path to the pre-trained word2vec model\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.search_kw = search_kw\n",
    "        self.model = None\n",
    "        self.search_words = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Load the word2vec model\n",
    "        self.model = KeyedVectors.load_word2vec_format(self.model_path, binary=True)\n",
    "        # Exract the words for the search keywords\n",
    "        self.search_words= self.get_words(self.search_kw)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Compute cosine similarity between each sentence in job tilte (X) and the search keywords\n",
    "        cosine_sim = [self.pairwise_cs(self.get_words(sentence), self.search_words) for _, sentence in X.items()]        \n",
    "        return np.array(cosine_sim).reshape(-1, 1)\n",
    "    \n",
    "\n",
    "    def get_words(self, sentence):\n",
    "        # Convert sentence to tokens, ignoring out-of-vocabulary words\n",
    "        words = [word for word in simple_preprocess(sentence) if word in list(self.model.index_to_key)]\n",
    "        return words\n",
    "\n",
    "    def cosine_similarity(self, vec1, vec2):\n",
    "        # Compute cosine similarity between two vectors\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    \n",
    "    def pairwise_cs(self, sentence1, sentence2):\n",
    "        if (sentence1 and sentence2): # So that both are not empty. If so, just return 0\n",
    "            cosine_vals = []\n",
    "            for  word1, word2 in list(itertools.product(sentence1, sentence2)):\n",
    "                cosine_vals.append(self.cosine_similarity(self.model[word1], self.model[word2]))\n",
    "            cosine_vals = np.array(cosine_vals)\n",
    "            # Only taking the mean of the top 20 %ile of cosine values and calculating the mean as the similarity between two sentences\n",
    "            return np.mean(cosine_vals[cosine_vals >= np.percentile( cosine_vals, 80) ])\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "# Now lets run the pipeline and get the fit information\n",
    "search_words = \"Human Resources in California\" # location information in the search words can improve predictions\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "\n",
    "        ('conn', FunctionTransformer(np.vectorize(categorize_connections),  validate=False), ['connection']),\n",
    "        ('loc', FunctionTransformer(location_scores, validate=False, kw_args={'search_words': search_words}) ,  'location'),\n",
    "        ('title', CosineSimilarityTransformerW2V(search_kw = search_words, model_path = WORD2VEC_PATH), 'job_title')\n",
    "    ] \n",
    ")\n",
    "\n",
    "# Weights of each score. Let us asssign an importance score of 0.2, 0.5 and 0.7 for information contained in their no of connections, location and title respectively\n",
    "# The values of no of connections can be from 0 to 3, and no of locations also has the same range, but the cosine similarity is between -1 and 1\n",
    "weights = np.array([0.2, 0.5, 0.7])\n",
    "talents_df_clean['fit'] = (preprocessor.fit_transform(talents_df_clean)*weights).sum(axis=1)\n",
    "\n",
    "#Displaying candidates in descending order of fit score\n",
    "talents_df_clean.sort_values(by =['fit'], ascending=False, inplace = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As anticipated, because HR and Human Resources mean the sane thing, by using the word2vec model,  so we have additional relevant results ranked higher. Job titles such as people development coordinater is also ranked higher as the job title is very relevant to HR, which all the previous models failed to capture as they were relying on same words and not word similarity in the semantic space. However, even this model has a disadvantage as we are dealing with word embeddings, i.e, looking at each word indiviudally and not capturng the  context the word is in. Sentence embeddings can better capture the information in a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Using sentence level representation from the [CLS] token from BERT and calculating the similarity score using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HR Senior Specialist</td>\n",
       "      <td>San Francisco Bay Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>2.203727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Nortia Staffing is seeking Human Resources, Pa...</td>\n",
       "      <td>San Jose, California</td>\n",
       "      <td>500+</td>\n",
       "      <td>2.179316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>HR Manager at Endemol Shine North America</td>\n",
       "      <td>Los Angeles, California</td>\n",
       "      <td>268</td>\n",
       "      <td>1.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>People Development Coordinator at Ryan</td>\n",
       "      <td>Denton, Texas</td>\n",
       "      <td>500+</td>\n",
       "      <td>1.766357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Human Resources Generalist at Schwan's</td>\n",
       "      <td>Amerika Birleşik Devletleri</td>\n",
       "      <td>500+</td>\n",
       "      <td>1.720010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Human Resources Coordinator at InterContinenta...</td>\n",
       "      <td>Atlanta, Georgia</td>\n",
       "      <td>500+</td>\n",
       "      <td>1.715050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Director Of Administration at Excellence Logging</td>\n",
       "      <td>Katy, Texas</td>\n",
       "      <td>500+</td>\n",
       "      <td>1.693956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Student at Chapman University</td>\n",
       "      <td>Lake Forest, California</td>\n",
       "      <td>2</td>\n",
       "      <td>1.631176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Admissions Representative at Community medical...</td>\n",
       "      <td>Long Beach, California</td>\n",
       "      <td>9</td>\n",
       "      <td>1.579031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Human Resources professional for the world lea...</td>\n",
       "      <td>Highland, California</td>\n",
       "      <td>50</td>\n",
       "      <td>1.575069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            job_title  \\\n",
       "7                                HR Senior Specialist   \n",
       "23  Nortia Staffing is seeking Human Resources, Pa...   \n",
       "31          HR Manager at Endemol Shine North America   \n",
       "3              People Development Coordinator at Ryan   \n",
       "26             Human Resources Generalist at Schwan's   \n",
       "11  Human Resources Coordinator at InterContinenta...   \n",
       "52   Director Of Administration at Excellence Logging   \n",
       "9                       Student at Chapman University   \n",
       "41  Admissions Representative at Community medical...   \n",
       "32  Human Resources professional for the world lea...   \n",
       "\n",
       "                       location connection       fit  \n",
       "7        San Francisco Bay Area      500+   2.203727  \n",
       "23         San Jose, California      500+   2.179316  \n",
       "31      Los Angeles, California        268  1.946667  \n",
       "3                 Denton, Texas      500+   1.766357  \n",
       "26  Amerika Birleşik Devletleri      500+   1.720010  \n",
       "11             Atlanta, Georgia      500+   1.715050  \n",
       "52                  Katy, Texas      500+   1.693956  \n",
       "9       Lake Forest, California          2  1.631176  \n",
       "41       Long Beach, California          9  1.579031  \n",
       "32         Highland, California         50  1.575069  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import torch\n",
    "\n",
    "class CosineSimilarityTransformerBERT(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, search_kw, model_name='bert-base-uncased') :\n",
    "        \"\"\"\n",
    "        Custom transformer to calculate cosine similarity \n",
    "        between the  key words and the job_titles segments using the BERT model\n",
    "\n",
    "        search_kw: str, the search key words against which to compute similarity\n",
    "        model_name: str, model name of the BERT model to use\n",
    "\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "        self.search_kw = search_kw\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "        self.kw_vec =  None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Calculating the sentence embedding of the search key words\n",
    "        inputs = self.tokenizer(self.search_kw , return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            self.kw_vec = self.model(**inputs).last_hidden_state[:, 0, :].squeeze()  # CLS token representation\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        # Transform each job_title in the DataFrame column to cosine similarity score with the search key words\n",
    "        cos_sim_scores = []\n",
    "        for  _, job_title in X.items():\n",
    "            inputs = self.tokenizer(job_title, return_tensors='pt', padding=True, truncation=True)\n",
    "            with torch.no_grad():\n",
    "                job_title_vec = self.model(**inputs).last_hidden_state[:, 0, :].squeeze()\n",
    "            cos_sim = torch.nn.functional.cosine_similarity(self.kw_vec, job_title_vec, dim=0)\n",
    "            cos_sim_scores.append(cos_sim.item())\n",
    "        return np.array(cos_sim_scores).reshape(-1, 1)\n",
    "    \n",
    "\n",
    "# Now lets run the pipeline and get the fit information\n",
    "search_words = \"Human Resources in California\" # location information in the search words can improve predictions\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "\n",
    "        ('conn', FunctionTransformer(np.vectorize(categorize_connections),  validate=False), ['connection']),\n",
    "        ('loc', FunctionTransformer(location_scores, validate=False, kw_args={'search_words': search_words}) ,  'location'),\n",
    "        ('title', CosineSimilarityTransformerBERT(search_kw = search_words, model_name='bert-base-uncased'), 'job_title')\n",
    "    ] \n",
    ")\n",
    "\n",
    "# Weights of each score. Let us asssign an importance score of 0.2, 0.5 and 0.7 \n",
    "# for information contained in their no of connections, location and title respectively\n",
    "# The values of no of connections can be from 0 to 3, and no of locations also has the same range, \n",
    "# but the cosine similarity is between -1 and 1\n",
    "\n",
    "weights = np.array([0.2, 0.5, 0.7])\n",
    "talents_df_clean['fit'] = (preprocessor.fit_transform(talents_df_clean)*weights).sum(axis=1)\n",
    "\n",
    "#Displaying candidates in descending order of fit score\n",
    "talents_df_clean.sort_values(by =['fit'], ascending=False, inplace = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Using SentenceTransformers  from Sentence-BERT and calculating the similarity score using cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will use Sentence BERT that was trained to get semantically meaningful sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HR Senior Specialist</td>\n",
       "      <td>San Francisco Bay Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>1.907255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Nortia Staffing is seeking Human Resources, Pa...</td>\n",
       "      <td>San Jose, California</td>\n",
       "      <td>500+</td>\n",
       "      <td>1.875520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>HR Manager at Endemol Shine North America</td>\n",
       "      <td>Los Angeles, California</td>\n",
       "      <td>268</td>\n",
       "      <td>1.690532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Human Resources Coordinator at InterContinenta...</td>\n",
       "      <td>Atlanta, Georgia</td>\n",
       "      <td>500+</td>\n",
       "      <td>1.542001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Human Resources Generalist at Schwan's</td>\n",
       "      <td>Amerika Birleşik Devletleri</td>\n",
       "      <td>500+</td>\n",
       "      <td>1.452684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Human Resources Management Major</td>\n",
       "      <td>Milpitas, California</td>\n",
       "      <td>18</td>\n",
       "      <td>1.409303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>People Development Coordinator at Ryan</td>\n",
       "      <td>Denton, Texas</td>\n",
       "      <td>500+</td>\n",
       "      <td>1.389546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Human Resources professional for the world lea...</td>\n",
       "      <td>Highland, California</td>\n",
       "      <td>50</td>\n",
       "      <td>1.316694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Seeking Human Resources Opportunities</td>\n",
       "      <td>Chicago, Illinois</td>\n",
       "      <td>390</td>\n",
       "      <td>1.311758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Director Of Administration at Excellence Logging</td>\n",
       "      <td>Katy, Texas</td>\n",
       "      <td>500+</td>\n",
       "      <td>1.268445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            job_title  \\\n",
       "7                                HR Senior Specialist   \n",
       "23  Nortia Staffing is seeking Human Resources, Pa...   \n",
       "31          HR Manager at Endemol Shine North America   \n",
       "11  Human Resources Coordinator at InterContinenta...   \n",
       "26             Human Resources Generalist at Schwan's   \n",
       "36                   Human Resources Management Major   \n",
       "3              People Development Coordinator at Ryan   \n",
       "32  Human Resources professional for the world lea...   \n",
       "13              Seeking Human Resources Opportunities   \n",
       "52   Director Of Administration at Excellence Logging   \n",
       "\n",
       "                       location connection       fit  \n",
       "7        San Francisco Bay Area      500+   1.907255  \n",
       "23         San Jose, California      500+   1.875520  \n",
       "31      Los Angeles, California        268  1.690532  \n",
       "11             Atlanta, Georgia      500+   1.542001  \n",
       "26  Amerika Birleşik Devletleri      500+   1.452684  \n",
       "36         Milpitas, California         18  1.409303  \n",
       "3                 Denton, Texas      500+   1.389546  \n",
       "32         Highland, California         50  1.316694  \n",
       "13            Chicago, Illinois        390  1.311758  \n",
       "52                  Katy, Texas      500+   1.268445  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "class CosineSimilarityTransformerSBERT(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self,  search_kw, model_name='all-MiniLM-L6-v2'):\n",
    "        \"\"\"\n",
    "        Custom transformer to calculate cosine similarity \n",
    "        between the  key words and the job_titles segments using the Sentence-BERT model\n",
    "\n",
    "        search_kw: str, the search key words against which to compute similarity\n",
    "        model_name: str, model name of the Sentence-BERT model to use\n",
    "\n",
    "        \"\"\"\n",
    "        # Initialize and loading the model information\n",
    "        self.model_name = model_name\n",
    "        self. search_kw =  search_kw\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.ref_vec  = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        # Precompute the search keywords embedding\n",
    "        self.ref_vec = self.model.encode(self.search_kw, convert_to_tensor=True)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Calculate cosine similarity of each job title to the search keywords\n",
    "        sentence_embeddings = self.model.encode(X, convert_to_tensor=True)\n",
    "        cosine_similarities = util.pytorch_cos_sim(self.ref_vec, sentence_embeddings).flatten()\n",
    "        return cosine_similarities.numpy().reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Now lets run the pipeline and get the fit information\n",
    "search_words = \"Human Resources in California\" # location information in the search words can improve predictions\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "\n",
    "        ('conn', FunctionTransformer(np.vectorize(categorize_connections),  validate=False), ['connection']),\n",
    "        ('loc', FunctionTransformer(location_scores, validate=False, kw_args={'search_words': search_words}) ,  'location'),\n",
    "        ('title', CosineSimilarityTransformerSBERT(search_kw = search_words, model_name='all-MiniLM-L6-v2'), 'job_title')\n",
    "    ] \n",
    ")\n",
    "\n",
    "# Weights of each score. Let us asssign an importance score of 0.2, 0.5 and 0.7 for information contained in their no of connections, location and title respectively\n",
    "# The values of no of connections can be from 0 to 3, and no of locations also has the same range, but the cosine similarity is between -1 and 1\n",
    "weights = np.array([0.2, 0.5, 0.7])\n",
    "talents_df_clean['fit'] = (preprocessor.fit_transform(talents_df_clean)*weights).sum(axis=1)\n",
    "\n",
    "#Displaying candidates in descending order of fit score\n",
    "talents_df_clean.sort_values(by =['fit'], ascending=False, inplace = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above the top 10 results are very relavent to the search keywords (Human resources in California) with all of them based in USA, and a significant chunk of them based in California. Hence it looks like Sentence-BERT is performing as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Updating the fit scores based on the starred candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any profile is starred then, we can modifying the reference search vector as a weighted sum of the keywords_search vector and the starred candidates job title vector, we can increase the cosine similarity between the updated reference search vector and similar candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the necessary transformations and functions\n",
    "\n",
    "from sentence_transformers import util\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from opencage.geocoder import OpenCageGeocode\n",
    "\n",
    "class CosineSimilarityTransformerSBERT2(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, reference_vec, model):\n",
    "        \"\"\"\n",
    "        Custom transformer to calculate cosine similarity \n",
    "        between the  key words and the job_titles segments using the Sentence-BERT model\n",
    "\n",
    "        reference_vec: the reference vec against which to compute similarity for all job titles\n",
    "        model: Sentence BERT model used\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = model\n",
    "        self.reference_vec  = reference_vec\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Calculate cosine similarity of each job title to the reference vector\n",
    "        \n",
    "        sentence_embeddings = self.model.encode(X, convert_to_tensor=True)\n",
    "        cosine_similarities = util.pytorch_cos_sim(self.reference_vec, sentence_embeddings).flatten()\n",
    "        return cosine_similarities.numpy().reshape(-1, 1)\n",
    "    \n",
    "\n",
    "# Calculating the location scores based on the location_df, if same city, then the score is 3,\n",
    "# if same state score is 2 and if same country then the score is 1, if none match, then zero\n",
    "\n",
    "def location_scores2(location_names, locations_df):\n",
    "\n",
    "    \"\"\"\n",
    "    uses  search keywords and information in the location column of a data frame to find similarity\n",
    " \n",
    "    Args:\n",
    "        location names : Data frame column  (pandas series) containing locatoin information for job candidates\n",
    "        locations_df :  Data frame containing city, county and state for all previously starred candiates and the search term\n",
    "\n",
    "    Returns:\n",
    "        pd.series: The calculated similarity scores between loc column and search string\n",
    "    \"\"\"\n",
    "\n",
    "    if not locations_df.empty:\n",
    "        scores = []\n",
    "        for location_name in location_names:\n",
    "            location_dict = get_location_info(location_name)\n",
    "            score = 0\n",
    "            if location_dict:\n",
    "                for i, loc_row in locations_df.iterrows():\n",
    "                    if (location_dict['country'] and (location_dict['country'] == loc_row['country'])):\n",
    "                        score += 1\n",
    "                        if (location_dict['state'] and (location_dict['state'] == loc_row['state'])):\n",
    "                            score += 1\n",
    "                            if (location_dict['city'] and (location_dict['city'] == loc_row['city'])): \n",
    "                                score += 1\n",
    "            scores.append(score)\n",
    "        scores = np.array(scores)\n",
    "        scaled_scores = (scores - scores.min(axis=0)) / (scores.max(axis =0) - scores.min(axis=0)) \n",
    "        # Scaling scores so that they are in a range between 0 and 1\n",
    "        return scaled_scores.reshape(-1, 1)\n",
    "    else:\n",
    "        return np.zeros(len(location_names)).reshape(-1, 1)\n",
    "\n",
    "# Encoding  LinkedIn connections so that they are always between 0 and 1\n",
    "def categorize_connections2(num:int, threshold:float = 0)-> int:\n",
    "    '''\n",
    "    Bins the no of LinkedIn connections to ordinal values based on threshold\n",
    "\n",
    "    ''' \n",
    "    if num:\n",
    "        if not threshold:\n",
    "            if num =='500+ ':\n",
    "                return 3/3\n",
    "            elif 0 <= int(num) < 100:\n",
    "                return 0/3\n",
    "            elif 100 <= int(num) < 200:\n",
    "                return 1/3\n",
    "            elif 200 <= int(num) < 500:\n",
    "                return 2/3\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            if num =='500+ ':\n",
    "                return 1\n",
    "            elif num >= threshold:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# Usign OpenCage Geocode API to get location information\n",
    "geocoder = OpenCageGeocode(GeocodingKey)\n",
    "\n",
    "# Finding and extacting location infromation from the keywords\n",
    "def get_location_info(location_name):\n",
    "       \n",
    "    \"\"\"\n",
    "    uses location name string and outputs the city state and country information for the top search result \n",
    "    for the location string as a dict after quertying the Open Cage Geocode API\n",
    "\n",
    "    Args:\n",
    "        location names : string containing location information\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containg the city, state and country inforation for the search string\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        results = geocoder.geocode(location_name, no_annotations='1')\n",
    "        keys = [\"city\", \"state\", \"country\"]\n",
    "        location = dict.fromkeys(keys)\n",
    "        if results:\n",
    "            # Usually, the first result is the most relevant one\n",
    "            top_result = results[0]['components']\n",
    "            for key in keys:\n",
    "                location[key] = top_result.get(key, None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error geocoding {location_name}: {e}\")\n",
    "        location = dict.fromkeys([\"city\", \"state\", \"country\"], None)\n",
    "    return location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HR Senior Specialist</td>\n",
       "      <td>San Francisco Bay Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>1.007255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Nortia Staffing is seeking Human Resources, Pa...</td>\n",
       "      <td>San Jose, California</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.975520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>HR Manager at Endemol Shine North America</td>\n",
       "      <td>Los Angeles, California</td>\n",
       "      <td>268</td>\n",
       "      <td>0.923865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Human Resources Management Major</td>\n",
       "      <td>Milpitas, California</td>\n",
       "      <td>18</td>\n",
       "      <td>0.909303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Human Resources Coordinator at InterContinenta...</td>\n",
       "      <td>Atlanta, Georgia</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.892001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Human Resources professional for the world lea...</td>\n",
       "      <td>Highland, California</td>\n",
       "      <td>50</td>\n",
       "      <td>0.816694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Human Resources Generalist at Schwan's</td>\n",
       "      <td>Amerika Birleşik Devletleri</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.802684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Seeking Human Resources Opportunities</td>\n",
       "      <td>Chicago, Illinois</td>\n",
       "      <td>390</td>\n",
       "      <td>0.795091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>People Development Coordinator at Ryan</td>\n",
       "      <td>Denton, Texas</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.739546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Aspiring Human Resources Professional | Passio...</td>\n",
       "      <td>New York, New York</td>\n",
       "      <td>212</td>\n",
       "      <td>0.729700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            job_title  \\\n",
       "7                                HR Senior Specialist   \n",
       "23  Nortia Staffing is seeking Human Resources, Pa...   \n",
       "31          HR Manager at Endemol Shine North America   \n",
       "36                   Human Resources Management Major   \n",
       "11  Human Resources Coordinator at InterContinenta...   \n",
       "32  Human Resources professional for the world lea...   \n",
       "26             Human Resources Generalist at Schwan's   \n",
       "13              Seeking Human Resources Opportunities   \n",
       "3              People Development Coordinator at Ryan   \n",
       "24  Aspiring Human Resources Professional | Passio...   \n",
       "\n",
       "                       location connection       fit  \n",
       "7        San Francisco Bay Area      500+   1.007255  \n",
       "23         San Jose, California      500+   0.975520  \n",
       "31      Los Angeles, California        268  0.923865  \n",
       "36         Milpitas, California         18  0.909303  \n",
       "11             Atlanta, Georgia      500+   0.892001  \n",
       "32         Highland, California         50  0.816694  \n",
       "26  Amerika Birleşik Devletleri      500+   0.802684  \n",
       "13            Chicago, Illinois        390  0.795091  \n",
       "3                 Denton, Texas      500+   0.739546  \n",
       "24           New York, New York        212  0.729700  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will test out the updating pipeline that can adjust according to the starred candidates\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "alpha = 0.3 # This is the parameter which modifies the reference search vector by weighting the starred candidate\n",
    "search_words = \"Human Resources in California\"\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') # Sentence-BERT model\n",
    "\n",
    "# Extracting information from the search words to get inital set of predictions\n",
    "# Deriving sentence embeddings for the search words\n",
    "reference_vec = model.encode(search_words, convert_to_tensor=True)\n",
    " \n",
    "# Extracting and saving any location information in the search keywords\n",
    "locations_df = pd.DataFrame(columns = ['country','state', 'city'])\n",
    "search_loc = [ent.text for ent in nlp(search_words).ents if ent.label_ == 'GPE']\n",
    "if search_loc:\n",
    "    search_loc_dict = get_location_info(' '.join(search_loc))\n",
    "    locations_df = locations_df._append(search_loc_dict,  ignore_index=True)\n",
    "\n",
    "# Thresholds for the categorizing the no of LinkedIn connections\n",
    "thresholds = [0]\n",
    "\n",
    "# Defining the transformer pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "\n",
    "        ('conn', FunctionTransformer(np.vectorize(categorize_connections2), validate=False, \n",
    "                                     kw_args={'threshold': np.mean(np.array(thresholds))}), ['connection']),\n",
    "        ('loc', FunctionTransformer(location_scores2, validate=False, kw_args={'locations_df': locations_df}) ,'location'),\n",
    "        ('title', CosineSimilarityTransformerSBERT2(reference_vec = reference_vec, model = model), 'job_title')\n",
    "    ] \n",
    ")\n",
    "\n",
    "# Weights of each score. Let us asssign an importance score of 0.2, 0.5 and 0.7 \n",
    "# for information contained in their no of connections, location and title respectively\n",
    "# The values of no of connections can be from 0 to 1, and no of locations also has the same range,\n",
    "#  but the cosine similarity is between -1 and 1\n",
    "\n",
    "weights = np.array([0.2, 0.5, 0.7])\n",
    "talents_df_clean['fit'] = (preprocessor.fit_transform(talents_df_clean)*weights).sum(axis=1)\n",
    "\n",
    "#Displaying candidates in descending order of fit score\n",
    "talents_df_clean.sort_values(by =['fit'], ascending=False, inplace = False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>HR Manager at Endemol Shine North America</td>\n",
       "      <td>Los Angeles, California</td>\n",
       "      <td>268</td>\n",
       "      <td>1.175953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HR Senior Specialist</td>\n",
       "      <td>San Francisco Bay Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.979793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Nortia Staffing is seeking Human Resources, Pa...</td>\n",
       "      <td>San Jose, California</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.927448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Human Resources Coordinator at InterContinenta...</td>\n",
       "      <td>Atlanta, Georgia</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.894896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Human Resources Management Major</td>\n",
       "      <td>Milpitas, California</td>\n",
       "      <td>18</td>\n",
       "      <td>0.862147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Seeking Human Resources Opportunities</td>\n",
       "      <td>Chicago, Illinois</td>\n",
       "      <td>390</td>\n",
       "      <td>0.845434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Aspiring Human Resources Professional | An ene...</td>\n",
       "      <td>Austin, Texas Area</td>\n",
       "      <td>174</td>\n",
       "      <td>0.822542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Human Resources Generalist at Schwan's</td>\n",
       "      <td>Amerika Birleşik Devletleri</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.789763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Aspiring Human Resources Professional | Passio...</td>\n",
       "      <td>New York, New York</td>\n",
       "      <td>212</td>\n",
       "      <td>0.771326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Human Resources professional for the world lea...</td>\n",
       "      <td>Highland, California</td>\n",
       "      <td>50</td>\n",
       "      <td>0.752812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            job_title  \\\n",
       "31          HR Manager at Endemol Shine North America   \n",
       "7                                HR Senior Specialist   \n",
       "23  Nortia Staffing is seeking Human Resources, Pa...   \n",
       "11  Human Resources Coordinator at InterContinenta...   \n",
       "36                   Human Resources Management Major   \n",
       "13              Seeking Human Resources Opportunities   \n",
       "30  Aspiring Human Resources Professional | An ene...   \n",
       "26             Human Resources Generalist at Schwan's   \n",
       "24  Aspiring Human Resources Professional | Passio...   \n",
       "32  Human Resources professional for the world lea...   \n",
       "\n",
       "                       location connection       fit  \n",
       "31      Los Angeles, California        268  1.175953  \n",
       "7        San Francisco Bay Area      500+   0.979793  \n",
       "23         San Jose, California      500+   0.927448  \n",
       "11             Atlanta, Georgia      500+   0.894896  \n",
       "36         Milpitas, California         18  0.862147  \n",
       "13            Chicago, Illinois        390  0.845434  \n",
       "30           Austin, Texas Area        174  0.822542  \n",
       "26  Amerika Birleşik Devletleri      500+   0.789763  \n",
       "24           New York, New York        212  0.771326  \n",
       "32         Highland, California         50  0.752812  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sindx = 31 # If it is the index selected\n",
    "# Extracting information from the job title to update the search _vector\n",
    "\n",
    "job_title_col = talents_df_clean.loc[sindx, 'job_title']\n",
    "reference_vec = (1-alpha)*reference_vec +(alpha* model.encode(job_title_col, convert_to_tensor=True))\n",
    "\n",
    "# Extracting information for updating locations\n",
    "location_col = talents_df_clean.loc[sindx, 'location']\n",
    "locations_df = locations_df._append(get_location_info(location_col),  ignore_index=True)\n",
    "\n",
    "# Extracting information for updating connections\n",
    "\n",
    "connections_col = talents_df_clean.loc[sindx, 'connection']\n",
    "if connections_col=='500+ ':\n",
    "    thresholds.append(500)\n",
    "else:\n",
    "    thresholds.append(int(connections_col))\n",
    "\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "transformers=[\n",
    "\n",
    "    ('conn', FunctionTransformer(np.vectorize(categorize_connections2), validate=False, kw_args={'threshold': np.mean(np.array(thresholds))}), ['connection']),\n",
    "    ('loc', FunctionTransformer(location_scores2, validate=False, kw_args={'locations_df': locations_df}) ,  'location'),\n",
    "    ('title', CosineSimilarityTransformerSBERT2(reference_vec = reference_vec, model = model), 'job_title')\n",
    "    ] \n",
    "    )\n",
    "\n",
    "transformed = preprocessor.fit_transform(talents_df_clean)\n",
    "weights = np.array([0.2, 0.5, 0.7])\n",
    "talents_df_clean['fit'] = (transformed*weights).sum(axis=1)\n",
    "talents_df_clean.sort_values(by =['fit'], ascending=False, inplace = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected since index 31 was starred, the fit score of it has improved and it ranks as the candidate with the highest fit score. Hence the model is updating based on the starred candidates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
